{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc68ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf5ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "df_main = pd.read_csv(\"C:/Users/Insight/Desktop/Python_analysis/startup_funding.csv\")\n",
    "df = df_main.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8967e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640043f",
   "metadata": {},
   "source": [
    "### Cleaning all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cdd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming column names for consistency\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.replace('  ', ' ', regex=False)\n",
    "    df.rename(columns = {'Sr No':'serial_no', 'Date dd/mm/yyyy':'disbursement_date', 'InvestmentnType':'investment_type'}, \n",
    "              inplace=True)\n",
    "    df.columns = df.columns.str.replace(' ', '_').str.lower()\n",
    "    return df\n",
    "\n",
    "# Apply the function to clean column names\n",
    "df = clean_column_names(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e06382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking columns with null values\n",
    "df.isnull().sum()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean()*100  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['remarks', 'subvertical']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b00bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Selecting columns with null values\n",
    "columns_to_fill = ['industry_vertical', 'city_location', 'investors_name', 'investment_type']\n",
    "\n",
    "#Replacing the blank and NaN columns with \"Unknown\" \n",
    "df[columns_to_fill] = df[columns_to_fill].fillna('Unknown')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cc1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of unwanted characters/patterns to remove\n",
    "unwanted_patterns = [r'\\\\xc2\\\\xa0', r'\\\\xc3\\\\x98', r'\\\\xe2\\\\x80\\\\x99', r'\\\\xc3\\\\xa9c', r'\\\\xe2\\\\x80\\\\x93', r'\\\\n', r'\\\\x',]\n",
    "\n",
    "#Combining patterns into one big regex pattern (handles all unwanted characters at a go)\n",
    "combined_pattern = re.compile('|'.join(unwanted_patterns))\n",
    "\n",
    "#Function to clean unwanted patterns and also handle any encoding issues\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = bytes(text, 'utf-8').decode('unicode_escape') #This decodes escape sequences (such as \\\\xc2\\\\xa0), unwanted patterns into their respective characters (e.g., the non-breaking space character). \n",
    "        text = combined_pattern.sub('', text) #Removing unwanted patterns\n",
    "        text = re.sub(r'\\s+', ' ', text).strip() #Removing extra spaces that might have been created during cleaning\n",
    "    return text\n",
    "\n",
    "#Applying the cleaning function to all columns\n",
    "df = df.applymap(clean_text)\n",
    "\n",
    "specific_rows = [2605, 2609, 2611, 2227, 1186, 141]\n",
    "df_selected = df.loc[specific_rows]\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa63f0a",
   "metadata": {},
   "source": [
    "### Cleaning the date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622774fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean and standardize dates\n",
    "\n",
    "def clean_date(date):\n",
    "    date = date.replace('.', '/').replace('//', '/') #Replacing incorrect separators\n",
    "    \n",
    "    #Fixing short years (e.g., '015' -> '2015')\n",
    "    if date.count('/') == 2:\n",
    "        day, month, year = date.split('/')\n",
    "        if len(year) < 4:\n",
    "            year = '20' + year[-2:]\n",
    "        date = f\"{day}/{month}/{year}\"\n",
    "    \n",
    "    #Handling joined month and year (e.g., '05/072018' -> '05/07/2018')\n",
    "    elif date.count('/') == 1 and len(date.split('/')[1]) == 6:\n",
    "        day, month_year = date.split('/')\n",
    "        month, year = month_year[:2], month_year[2:]\n",
    "        date = f\"{day}/{month}/{year}\"\n",
    "\n",
    "    #Converting to datetime and format as dd/mm/yyyy\n",
    "    return pd.to_datetime(date, errors='coerce', dayfirst=True).strftime('%d/%m/%Y')\n",
    "\n",
    "#Applying the function to clean and format the dates\n",
    "df['disbursement_date'] = df['disbursement_date'].apply(clean_date)\n",
    "\n",
    "specific_rows = [2571, 192, 2775, 2776, 3029, 2831, 3011]\n",
    "df_selected = df.loc[specific_rows]\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6ac4f",
   "metadata": {},
   "source": [
    "### Cleaning the startup_name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65567ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting column\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "value_counts = df['startup_name'].value_counts().sort_index()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f3a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Extracting the domain name from URL\n",
    "#Function to extract domain name if the entry is a URL and capitalize the first letter\n",
    "# Define the function for cleaning startup names\n",
    "def clean_startup_name(value):\n",
    "    # Remove non-ASCII characters\n",
    "    value = value.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Standardize specific patterns like \"Byju's\"\n",
    "    value = re.sub(r\"\\\"?Byju\\\\?'?S\\\"?\", \"Byjus\", value, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Check if the value is a URL\n",
    "    if value.startswith('http'):\n",
    "        domain_name = urlparse(value).netloc.split('.')[1].capitalize()  # Extract and capitalize domain\n",
    "        return domain_name\n",
    "    \n",
    "    # Remove specific patterns for non-URL values\n",
    "    cleaned_value = re.sub(r'\\.com|\\.co|\\.in|\\.ai|\\.IO', '', value, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Convert to title case\n",
    "    return cleaned_value.title()\n",
    "\n",
    "# Apply the combined cleaning function to the 'startup_name' column\n",
    "df['startup_name'] = df['startup_name'].apply(clean_startup_name)\n",
    "df.head()\n",
    "\n",
    "# Select specific rows\n",
    "specific_rows = [2269, 2873, 1677, 1411]\n",
    "df_selected = df.loc[specific_rows]\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "startup_mapping = {\n",
    "    '1Mg (Healthkartplus)': '1Mg', 'Aahaa': 'Aahaa Stores', 'Absentia': 'Absentia Vr', 'Active': 'Active Ai',\n",
    "    '#Fame': 'Fame', 'Arcatron': 'Arcatron Mobility', 'Ampere': 'Ampere Vehicles', 'Atomberg': 'Atomberg Technology',\n",
    "    'Availfinance': 'Avail Finance', 'Ayefinance': 'Aye Finance', 'Appdaily': 'Appsdaily', 'Bank Bazaar': 'Bankbazaar',\n",
    "    'Bhive': 'Bhive Workspace', 'Bhiveworkspace': 'Bhive Workspace', 'BigBasket': 'Bigbasket',\n",
    "    'Black White Orange': 'Black White Orange Brands', 'Buildkar': 'Buildzar', 'BYJUs': 'Byjus', 'BYJUS': 'Byjus',\n",
    "    'CCavenue': 'CCAvenue', 'Capillary': 'Capillary Tech', 'Caravan Craft Retail': 'Caravan Craft',\n",
    "    'Chaipoint': 'Chai Point', 'Cloudcherry': 'Cloudcherry Analytics', 'Confirmttkt': 'Confirmtkt',\n",
    "    'Craftstvilla': 'Craftsvilla', 'Crown-It': 'Crownit', 'Cult': 'Cult Fitness', 'Cure Fit': 'Curefit',\n",
    "    'Daily Hunt': 'Dailyhunt', 'Daily Rounds': 'Dailyrounds', 'Deal 4Loans': 'Deal4Loans', 'Dhruva': 'Dhruva Space',\n",
    "    'Doctorinsta': 'Doctor Insta', 'Early Salary': 'Earlysalary', 'Entropika': 'Entropik', 'Fab Hotels': 'Fabhotels',\n",
    "    'Fyle Technologies': 'Fyle', 'Gibbs': 'Gibss', 'Glamstudios': 'Glam Studios', 'Hansel Io': 'Hansel',\n",
    "    'Happilyunmarried': 'Happily Unmarried', 'Healthcare': 'Healthcare At Home', 'High Radius': 'Highradius',\n",
    "    'Hike Messenger': 'Hike', 'Hwell24': 'Hwell24 Plus', 'I2I Funding': 'I2Ifunding', 'Impact Guru': 'Impactguru',\n",
    "    'Incred': 'Incred Finance', 'Ink Monk': 'Inkmonk', 'Intelligencenode': 'Intelligence Node',\n",
    "    'Jollyfoodfellow': 'Jolly Food Fellow', 'Legal Raasta': 'Legalraasta', 'Lenden Club': 'Lendenclub',\n",
    "    'Lending Kart': 'Lendingkart', 'Little Black Book Delhi': 'Little Black Book', 'Loan Tap': 'Loantap',\n",
    "    'Log 9 Materials': 'Log9 Materials', 'Logicroots': 'Logic Roots', 'Milk Basket': 'Milkbasket', \n",
    "    'Nearbuy (Previously Groupon India)': 'Nearbuy', 'Neogrowth Credit': 'Neogrowth', 'Olacabs': 'Ola Cabs',\n",
    "    'Oyorooms': 'Oyo Rooms', 'Oyo': 'Oyo Rooms', 'Paytm Marketplace': 'Paytm', 'Pinelabs': 'Pine Labs', \n",
    "    'Pipabella': 'Pipa Bella', 'Policy Bazaar': 'Policybazaar', 'Pressplay Tv': 'Pressplay', 'Qyk App': 'Qyk', \n",
    "    'Rapido Bike Taxi': 'Rapido', 'Rawpressery':'Raw Pressery', 'Renew Buy': 'Renewbuy', 'Rentmojo': 'Rentomojo',\n",
    "    'Rollmafia': 'Roll Mafia', 'Satvakart': 'Satvacart', 'Shabdnagari': 'Shabdanagari', 'Shubhloans': 'Shubh Loans',\n",
    "    'Silvan': 'Silvan Innovation Labs', 'Samunnati Financial Intermediation & Services Pvt. Ltd': 'Samunnati',\n",
    "    'Spares Hub': 'Spareshub', 'Sport Flashes': 'Sports Flashes', 'Stalk Buy Love': 'Stalkbuylove',\n",
    "    'Staydobe': 'Stayabode', 'Tails Life': 'Tailslife', 'Tempgo': 'Tempogo', 'The Mons Co.': 'The Moms Co',\n",
    "    'Toko Innovations': 'Toko Innovation Studios', 'Tone Tag': 'Tonetag', 'Ue Life Sciences': 'Ue Lifesciences',\n",
    "    'Unaacademy': 'Unacademy', 'Urbanclap Technologies Pvt. Ltd': 'Urbanclap', 'Vahdam Tea': 'Vahdam Teas',\n",
    "    'Veritas Finance': 'Veritas Finance Ltd.', 'Vogo': 'Vogo Automotive', 'Vogo Automotive Pvt. Ltd.': 'Vogo Automotive',\n",
    "    'Zippserve': 'Zippserv', 'Zolo': 'Zolostays'   \n",
    "}\n",
    "df['startup_name'] = df['startup_name'].replace(startup_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1fa708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['startup_name'].value_counts().sort_index()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b145cb",
   "metadata": {},
   "source": [
    "### Cleaning the city_location column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e27cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['city_location'].value_counts().sort_index()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6019d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mapping = {\n",
    "    \"Ahemadabad\": \"Ahmedabad\",\n",
    "    \"Ahemdabad\": \"Ahmedabad\",\n",
    "    \"Andheri\": \"Mumbai\",\n",
    "    \"Bangalore\": \"Bengaluru\",\n",
    "    \"Bangalore / Palo Alto\": \"Bengaluru\",\n",
    "    \"Bangalore / SFO\": \"Bengaluru\",\n",
    "    \"Bangalore / San Mateo\": \"Bengaluru\",\n",
    "    \"Bangalore / USA\": \"Bengaluru\",\n",
    "    \"Bangalore/ Bangkok\": \"Bengaluru\",\n",
    "    \"Belgaum\": \"Bengaluru\",\n",
    "    \"Bengaluru and Gurugram\": \"Bengaluru\",\n",
    "    \"Bhubneswar\":\"Bhubaneswar\",\n",
    "    \"Chembur\": \"Mumbai\",\n",
    "    \"Chennai/ Singapore\": \"Chennai\",\n",
    "    \"Dallas / Hyderabad\": \"Hyderabad\",\n",
    "    \"Delhi & Cambridge\": \"Delhi\",\n",
    "    \"Goa/Hyderabad\": \"Goa\",\n",
    "    \"Gurgaon\": \"Gurugram\",\n",
    "    \"Gurgaon / SFO\": \"Gurugram\",\n",
    "    \"Hyderabad/USA\": \"Hyderabad\",\n",
    "    \"India / US\": \"India\",\n",
    "    \"India/Singapore\": \"India\",\n",
    "    \"India/US\": \"India\",\n",
    "    \"Karnataka\": \"Bengaluru\",\n",
    "    \"Kochi\": \"Kerala\",\n",
    "    \"Kolkatta\": \"Kolkata\",\n",
    "    \"Kormangala\": \"Bengaluru\",\n",
    "    \"Kozhikode\": \"Kerala\",\n",
    "    \"Menlo Park\": \"California\",\n",
    "    \"Mumbai/Bengaluru\": \"Mumbai\",\n",
    "    \"Mumbai / Global\": \"Mumbai\",\n",
    "    \"Mumbai / NY\": \"Mumbai\",\n",
    "    \"Mumbai / UK\": \"Mumbai\",\n",
    "    \"New Delhi / California\": \"New Delhi\",\n",
    "    \"New Delhi / US\": \"New Delhi\",\n",
    "    \"New Delhi/ Houston\": \"New Delhi\",\n",
    "    \"Nw Delhi\": \"New Delhi\",\n",
    "    \"New York\": \"New York\",\n",
    "    \"New York, Bengaluru\": \"New York\",\n",
    "    \"New York/ India\": \"New York\",\n",
    "    \"Noida / Singapore\": \"Noida\",\n",
    "    \"Palo Alto\": \"California\",\n",
    "    \"Panaji\": \"Goa\",\n",
    "    \"Pune / Dubai\": \"Pune\",\n",
    "    \"Pune / Singapore\": \"Pune\",\n",
    "    \"Pune / US\": \"Pune\",\n",
    "    \"Pune/Seattle\": \"Pune\",\n",
    "    \"San Francisco\": \"California\",\n",
    "    \"San Jose,\": \"California\",\n",
    "    \"Santa Monica\": \"California\",\n",
    "    \"SFO / Bangalore\": \"Bengaluru\", \n",
    "    \"Seattle / Bangalore\": \"Bengaluru\",\n",
    "    \"Taramani\": \"Chennai\",\n",
    "    \"Trivandrum\": \"Kerala\",\n",
    "    \"US\": \"USA\",\n",
    "    \"US/India\": \"USA\",\n",
    "    \"USA/India\": \"USA\"\n",
    "}\n",
    "df['city_location'] = df['city_location'].replace(city_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['city_location'].value_counts().sort_index()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf6e28",
   "metadata": {},
   "source": [
    "### Cleaning the investment_type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting column\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "value_counts = df['investment_type'].value_counts().sort_index()\n",
    "#print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_mapping = {\n",
    "    'Angel': 'Angel Funding',\n",
    "    'Angel / Seed Funding': 'Angel Seed Funding',\n",
    "    'Angel Round': 'Angel Funding',\n",
    "    'Bridge Round': 'Bridge Funding',\n",
    "    'Corporate Round': 'Corporate Funding',\n",
    "    'Crowd Funding': 'Crowdfunding',\n",
    "    'Crowd funding': 'Crowdfunding',\n",
    "    'Debt': 'Debt Funding',\n",
    "    'Debt-Funding': 'Debt Funding',\n",
    "    'Debt and Preference capital': 'Debt Funding',\n",
    "    'Equity': 'Equity Funding',\n",
    "    'Equity Based Funding': 'Equity Funding',\n",
    "    'Funding Round': 'Other Funding',\n",
    "    'Inhouse Funding': 'Other Funding',\n",
    "    'Maiden Round': 'Other Funding',\n",
    "    'Mezzanine': 'Other Funding',\n",
    "    'Pre Series A': 'Pre-Series A',\n",
    "    'pre-series A': 'Pre-Series A',\n",
    "    'pre-Series A': 'Pre-Series A',\n",
    "    'Pre-series A': 'Pre-Series A',\n",
    "    'Private': 'Private Equity',\n",
    "    'Private Equity Round': 'Private Equity',\n",
    "    'Private Funding': 'Private Equity',\n",
    "    'PrivateEquity': 'Private Equity',\n",
    "    'Seed': 'Seed Funding',\n",
    "    'Seed / Angel Funding': 'Angel Seed Funding',\n",
    "    'Seed / Angle Funding': 'Angel Seed Funding',\n",
    "    'Seed Funding Round': 'Seed Funding',\n",
    "    'Seed Round': 'Seed Funding',\n",
    "    'Seed funding': 'Seed Funding',\n",
    "    'Seed/ Angel Funding': 'Angel Seed Funding',\n",
    "    'Seed/Angel Funding': 'Angel Seed Funding',\n",
    "    'SeedFunding': 'Seed Funding',\n",
    "    'Series B (Extension)':'Series B',\n",
    "    'Single Venture': 'Venture Funding',\n",
    "    'Structured Debt': 'Debt Funding',\n",
    "    'Term Loan': 'Debt Funding',\n",
    "    'Venture': 'Venture Funding',\n",
    "    'Venture - Series Unknown': 'Venture Funding',\n",
    "    'Venture Round': 'Venture Funding'\n",
    "}\n",
    "\n",
    "df['investment_type'] = df['investment_type'].replace(funding_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['investment_type'].value_counts().sort_index()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5e7ac",
   "metadata": {},
   "source": [
    "### Cleaning the amount column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55126da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_amount(amount):\n",
    "    amount = str(amount).strip().lower() #Converting to string and stripping spaces\n",
    "    if amount in ['n/a', 'undisclosed', 'unknown']: \n",
    "        return 0.0\n",
    "\n",
    "    #Regex to remove unwanted characters (except for digits, decimal point, and minus sign)\n",
    "    amount = re.sub(r'[^0-9.-]', '', amount)\n",
    "    try:\n",
    "        cleaned_amount = float(amount)\n",
    "    except ValueError:\n",
    "        cleaned_amount = 0.0\n",
    "    return cleaned_amount\n",
    "\n",
    "df['amount_in_usd'] = df['amount_in_usd'].apply(clean_amount)\n",
    "\n",
    "#Formatting the output to avoid scientific notation and keeping it as float\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383b98e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "value_counts = df['amount_in_usd'].value_counts().sort_index()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f9097",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ccbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/Insight/Desktop/cleaned_data.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
